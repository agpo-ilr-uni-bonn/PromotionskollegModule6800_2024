{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agpo-ilr-uni-bonn/PromotionskollegModule6800_2024/blob/master/6800_Day3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3dbMPsC8kYb"
      },
      "source": [
        "# Day 3: Code used during lecture and lab assignment\n",
        "\n",
        "## Instructions\n",
        "\n",
        "- The notebook combines 'code used during lecture' with the corresponding lab assignment (see further down)\n",
        "- The lab assignment can be done largely by copying/paste/modification of the code used during the lecture\n",
        "- Please add answers/discussion/comments to the notebook as comments or text box. Do not create another file in addition.\n",
        "- When you are done with your assignment, save the notebook in drive and add your last name to the name of the file.\n",
        "- To hand in the final notebook follow the instructions provided by email"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku0lCEownFJh"
      },
      "source": [
        "### Notes:\n",
        "- The intention of the NN part of the notebook is somewhat different from the notebooks for the other days. Here, we do not aim for fitting an optimal model. The steps that we take here are not an illustration of how you would actually approach an estimation task. Instead we want to play around with a NN in serveral ways to build your understanding of how NN work.\n",
        "- We begin with a NN used for autoencoding and then move towards using a NN for prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQLmu7uoNE7h"
      },
      "source": [
        "### Code used during lecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QadP7iYo9N4M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sc\n",
        "import datetime, os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Set the numpy random seed\n",
        "np.random.seed(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEfL_HEFkr02"
      },
      "source": [
        "### Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcF4Tis482HQ"
      },
      "outputs": [],
      "source": [
        "# run this cell only once if you don't have wget installed\n",
        "# its assumed you are using windows and have python installed\n",
        "# only needed if you are running the notebook locally\n",
        "# %pip install wget\n",
        "#if not os.path.isfile('brazil_all_data_v2.gz'):\n",
        "#    !python -m wget  https://ilr-ml.s3.eu-central-1.amazonaws.com/brazil_all_data_v2.gz\n",
        "# Download data only once and make sure it is in the same folder as the notebook\n",
        "\n",
        "# check if brazil_all_data_v2.gz is available in the current folder and if not, download it\n",
        "\n",
        "if not os.path.isfile('brazil_all_data_v2.gz'):\n",
        "    !wget  https://ilr-ml.s3.eu-central-1.amazonaws.com/brazil_all_data_v2.gz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pjyhe3gvEvUr"
      },
      "outputs": [],
      "source": [
        "# load data into dataframe\n",
        "df = pd.read_parquet('brazil_all_data_v2.gz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAmje5rJAj9O"
      },
      "outputs": [],
      "source": [
        "# Define binary variable for deforestration in 2018 as in lab day 2\n",
        "df['D_defor_2018'] = df['defor_2018']>0\n",
        "Y_all = df['D_defor_2018']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zieYJbMh-mLe"
      },
      "outputs": [],
      "source": [
        "## Prepare the data\n",
        "# list(df.columns[~df.columns.str.contains('2018')])\n",
        "# Variables from day 2\n",
        "lstX = [\n",
        "  'wdpa_2017',\n",
        "  'population_2015',\n",
        "  'chirps_2017',\n",
        "  'defor_2017',\n",
        "  'maize',\n",
        "  'soy',\n",
        "  'sugarcane',\n",
        "  'perc_treecover',\n",
        "  'perm_water',\n",
        "  'travel_min',\n",
        "  'cropland',\n",
        "  'mean_elev',\n",
        "  'sd_elev',\n",
        "  'near_road',\n",
        "  'defor_2017_lag_1st_order',\n",
        "  'wdpa_2017_lag_1st_order',\n",
        "  'chirps_2017_lag_1st_order',\n",
        "  'population_2015_lag_1st_order',\n",
        "  'maize_lag_1st_order',\n",
        "  'soy_lag_1st_order',\n",
        "  'sugarcane_lag_1st_order',\n",
        "  'perc_treecover_lag_1st_order',\n",
        "  'perm_water_lag_1st_order',\n",
        "  'travel_min_lag_1st_order',\n",
        "  'cropland_lag_1st_order',\n",
        "  'mean_elev_lag_1st_order',\n",
        "  'sd_elev_lag_1st_order',\n",
        "  'near_road_lag_1st_order',\n",
        " ]\n",
        "\n",
        "X_all = df.loc[:,lstX]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGbT8-7kAfIt"
      },
      "outputs": [],
      "source": [
        "# Split the data into train and test data using sklearn train_test_split object\n",
        "#   (see: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
        "\n",
        "#   Note: This randomly split the data in 80% train and 20% test data\n",
        "X_train_raw, X_test_raw, Y_train, Y_test = train_test_split(X_all, Y_all, test_size = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msCExuk8A0QF"
      },
      "outputs": [],
      "source": [
        "# MinMax scale data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scalerX = MinMaxScaler()\n",
        "X_train = scalerX.fit_transform(X_train_raw)\n",
        "X_test = scalerX.transform(X_test_raw)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hS0JSHtcmWGU"
      },
      "outputs": [],
      "source": [
        "# Define helper function that we will use below to print the stats of each model\n",
        "def printOutput(Y_score,Y_true):\n",
        "\n",
        "  # Get true positive and false positive rate\n",
        "  # See: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
        "  fpr_Lg, tpr_Lg, _ = roc_curve(Y_true, Y_score)\n",
        "\n",
        "  # Get the Area under the cureve (AUC)\n",
        "  # See: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html\n",
        "  roc_auc_Lg = auc(fpr_Lg, tpr_Lg)\n",
        "\n",
        "  print('\\nROC AUC', roc_auc_Lg)\n",
        "\n",
        "  # Plot the ROC curve\n",
        "  plt.figure()\n",
        "  lw = 2\n",
        "  plt.plot(fpr_Lg, tpr_Lg, color='darkorange',\n",
        "          lw=lw, label='ROC curve (area = %0.2f' % roc_auc_Lg)\n",
        "  plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.title('Receiver operating characteristic example')\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQGV2AW0Z-Us"
      },
      "source": [
        "# Outline for NN part\n",
        "\n",
        "\n",
        "- Start with PCA to do dimensionality reduction. Use the encoded data to run\n",
        "  a logistic regression\n",
        "- Train a autoencoder\n",
        "- Build an encoder/decoder model from the autoencoder\n",
        "- Use the encoder to encode the test data and use the decoder to transform\n",
        "  it back to the original space => See how much information is lost\n",
        "- Use the encoder to encode the data from the input space in a lower dim\n",
        "  encoding space. Use this encoding to run a logistic regression\n",
        "  (similarly as with the PCA)\n",
        "- Lets do exaclty the same thing but now using a NN setting where we use the\n",
        "  autoencoder as a pretrained first layer and we train only the output layer\n",
        "  (this is basically the same as in the step befor)\n",
        "- *Now lets do everything in one step. Let the NN train all parameters,\n",
        "  either completly end-to-end or using the encoder layer from the autoencoder\n",
        "  as a starting point*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lkd4LHKlPFoX"
      },
      "source": [
        "# Step 1: Illustration Principal Component Analysis (PCA)\n",
        "Before looking into neural networks lets see how we could use Principal Component Analysis for our forest data.\n",
        "\n",
        "We will not look into the detailes of PCA; the only important point is to understand that PCA is a dimensionality reduction technique that transforms data from a higher dimensional space to a lower dimensional space while trying to limit the loss of information.\n",
        "\n",
        "Note: Above we defined an input feature matrix with 28 veriables (features) and on day 2 you saw that it is perfectly possible\\feasible to run a logisitic regression with all 28 variables. Hence a dimensionality reduction is not necessary here. Nevertheless, we use this example in order to illustrate how PCA and then following NN autoencoding work. A real world setup where such a dimenstionality reduction approach makes actual sense would involve a sgnificantly larger number of variables k.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eV7iV0P_BzC"
      },
      "outputs": [],
      "source": [
        "# Set the numpy random seed\n",
        "np.random.seed(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5F-B2oMeNn0H"
      },
      "outputs": [],
      "source": [
        "# Fit an PCA\n",
        "encoding_dim = 8 # map input data to a 8 dimensional space\n",
        "pca = PCA(n_components=encoding_dim)\n",
        "pca.fit(X_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plHWZmSBPTeO"
      },
      "outputs": [],
      "source": [
        "# Now we can use the trained PCA to encode our input data in the\n",
        "# lower dimensional space\n",
        "encode_PCA = pca.transform(X_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4BRmQf7uUCR"
      },
      "outputs": [],
      "source": [
        "print('Dim of feature matrix',X_train.shape)\n",
        "print('Dim of encoding matrix',encode_PCA.shape)\n",
        "print( 'Explained variance ratio', pca.explained_variance_ratio_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fFTrXGfOduk"
      },
      "outputs": [],
      "source": [
        "# We can use this lower dimensional encoding in a logistic regression\n",
        "# This means our logistic regression now does not have 28 dimensions as our\n",
        "# features matrix but only 8 dimensions (=encoding_dim)\n",
        "\n",
        "# Fit a logistic regression using the PCA encoding\n",
        "# Create the model object\n",
        "modelLgPCA = LogisticRegression(random_state=0,penalty='none',fit_intercept=True, max_iter=1000)\n",
        "# Fit the model using the training data\n",
        "modelLgPCA.fit(encode_PCA, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00yWK21Emvw2"
      },
      "outputs": [],
      "source": [
        "# Get the predicted probabiltities\n",
        "Y_score = modelLgPCA.decision_function(pca.transform(X_test))\n",
        "\n",
        "printOutput(Y_score,Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTIcPyu0m6As"
      },
      "outputs": [],
      "source": [
        "# If you compare this result with the logit regression from day 2 using all\n",
        "# features you can see that this is clearly worse. However, it is still a model\n",
        "# is not completly uselss... so it seems that out 8-dimensional encoding\n",
        "# contains something usefull."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOhYLk37pzS2"
      },
      "source": [
        "# Step 2: Lets see how we can achive the same using an Autoencoder\n",
        "\n",
        "For etstimating NN we use the libary \"Keras\" which is one of the most popular deep learning libaries (https://keras.io/). Keras is a wrapper for Tensorflow (https://www.tensorflow.org/) which allows to specify a tensorflow model in few commands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqXR-3mdumyV"
      },
      "outputs": [],
      "source": [
        "# Load keras and tesorflow\n",
        "import tensorflow as tf\n",
        "import keras as ke\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hx7eT-2x-p_s"
      },
      "outputs": [],
      "source": [
        "## Setup Autoencode as an Keras model\n",
        "# This example is adopted based on: https://blog.keras.io/building-autoencoders-in-keras.html\n",
        "\n",
        "# Set dimensionality of the encoding space.\n",
        "# As in the PCA example we want a 8 dim encoding space.\n",
        "encoding_dim = 8\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "print('Encoding Dim is equal to:', encoding_dim)\n",
        "print('Input Dim is equal to:', input_dim)\n",
        "\n",
        "# In Keras we can specify a NN layer in one line of code. This is what\n",
        "# we use here to specify an...\n",
        "\n",
        "# 1) input layer that has the dimension equal to the number of variables\n",
        "#    in the input (here =28)\n",
        "input_dat = Input(shape=(input_dim,))\n",
        "\n",
        "# 2) the second layer is the encoding layer. It takes the output from the\n",
        "#    input layer (\"input_dat\") as input and is a \"dense\" layer with\n",
        "#    \"encoding_dim\" (=8) neurons. It uses the \"relu\" as activation\n",
        "encoded = Dense(encoding_dim, activation='relu')(input_dat)\n",
        "\n",
        "# 3) The third layer is the decoding layer, which is our output layer.\n",
        "#    It takes the output from the encoding layer as input (\"encoded\"). It has\n",
        "#    \"input_dim\"=28 neurons. The acitvation function is not specified here, which\n",
        "#   means that we use the default activation function [identity function].\n",
        "decoded = Dense(input_dim)(encoded)\n",
        "\n",
        "# Using these layer we build a NN in tensorflow. We do this be passing the\n",
        "# input layer and the last layer to the keras \"Model\" function.\n",
        "# This is sufficient for Keras to now how to build the complete model.\n",
        "# (The information how the hidden layers should look like, is know because\n",
        "# we passed this as input to the output layer)\n",
        "autoencoder = Model(input_dat, decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukZfiU1DL2iG"
      },
      "outputs": [],
      "source": [
        "# Have a look how the model looks like\n",
        "# Our input layer does not have any paramters. This is simply a placeholder\n",
        "# for our data that we will pass to the model.\n",
        "# The encoding layer has 232 neuros = 28 (input) x 8 (output) + 8 (constants)\n",
        "# The decoding layer has 252 neuros = 8 (input) x 28 (output) + 28 (constants)\n",
        "# In total we therefore have 484 parameters. Already quite a lot for such a\n",
        "# small model!\n",
        "autoencoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpNF6tHUKLag"
      },
      "outputs": [],
      "source": [
        "# Now we tell Keras/tensorflow which optimization algorithm we want to use\n",
        "# We also need to define a learning rate. This might not be the optimal choice\n",
        "# here. It we could tune this parameter to obtain better results\n",
        "learning_rate = 0.001\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate)\n",
        "\n",
        "# We also need to define the type of loss function we would to considere.\n",
        "# Since we have a regression task we use MSE.\n",
        "# With this we can now compile the model.\n",
        "autoencoder.compile(optimizer=optimizer, loss='MeanSquaredError')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKyhh5IyAX0F"
      },
      "outputs": [],
      "source": [
        "# Finally we can start training our Auotencoder using the \"fit()\" function\n",
        "# - we pass the training data (X_train, X_train)\n",
        "# - we specify the number fo epochs, how often we want to move thorugh\n",
        "#   the data for training\n",
        "# - we specify the size of the minibatches\n",
        "# - we specify that we want the shuffle the data such that the order is changed\n",
        "#   in each epoch\n",
        "# - we also pass the test data for validation\n",
        "history = autoencoder.fit(\n",
        "                X_train, X_train,\n",
        "                epochs=30,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(X_test, X_test),\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGgxfoZnq8Oa"
      },
      "outputs": [],
      "source": [
        "# Let plot the Traning and validation Loss in order to see if our model\n",
        "# overfitts\n",
        "plt.plot(range(1, len(history.history['loss']) + 1), history.history['loss'],'r', label='Training Loss')\n",
        "plt.plot(range(1, len(history.history['val_loss']) + 1), history.history['val_loss'],'b', label='Validation Loss')\n",
        "plt.title('Training and validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5--WdpPIxP_"
      },
      "source": [
        "# Step 3a: Use the trained autoencoder to build an encoder and decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Y1m9gM5Bch3"
      },
      "outputs": [],
      "source": [
        "# Now we can use the trained encoder/decoder layer of our autoencoder\n",
        "# and build seperated encoder and decoder networks\n",
        "\n",
        "\n",
        "# Using only the first layer of the Autoencoder we can build an encoder model\n",
        "# This model maps an input to its encoded representation\n",
        "# Use again the Keras \"Model\" function passin again the input layer and now\n",
        "# the encoder layer as a output (note that by now the encoder layer is trained)\n",
        "encoder = Model(input_dat, encoded)\n",
        "\n",
        "# Have a look how the model looks like\n",
        "# This is basically the first halb of our autoencoder from above\n",
        "encoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGl3djoDCD0N"
      },
      "outputs": [],
      "source": [
        "# Additionally lets build a decoder model that can take an encoded input\n",
        "# and outputs the original variables\n",
        "\n",
        "# create a placeholder for an the encoded input with dimension\n",
        "# equal to the encoding dimension\n",
        "encoded_input = Input(shape=(encoding_dim,))\n",
        "\n",
        "# Get the last layer of the autoencoder model\n",
        "decoder_layer = autoencoder.layers[-1]\n",
        "\n",
        "# create the decoder model using the last layer\n",
        "# (the output layer of the autoencoder)\n",
        "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
        "\n",
        "# Have a look how the model looks like\n",
        "# This is basically the second half of our autoencoder\n",
        "decoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQAjp3sMCMbN"
      },
      "outputs": [],
      "source": [
        "# Lets see what we can do with these two models...\n",
        "# Lets look at one specific observation (the first observation)\n",
        "encoded_dat = encoder.predict(X_test[[0],:])\n",
        "# This is the encoded data for the first observation (with 8 dimensions)\n",
        "# This does not really have an interpreation but we can see how much\n",
        "# information is in this encoding by using it to reconstruct the\n",
        "# original values using our decoder... (see next cell)\n",
        "encoded_dat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpOsrNmDCRBk"
      },
      "outputs": [],
      "source": [
        "# This is the first observation after encoding and then decoding again.\n",
        "# (compare this to the orignal values, next cell...)\n",
        "decoded_dat = decoder.predict(encoded_dat)\n",
        "decoded_dat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Nq3hzxuCg4V"
      },
      "outputs": [],
      "source": [
        "# This are the original values of the first observation\n",
        "# This does not look to bad!\n",
        "X_test[[0],:]\n",
        "\n",
        "# Make sure that you understand what is happening here. This shows that to some\n",
        "# extent we are able to compress the information in the 28dim input vector in\n",
        "# something 8dim and then reconstruct the orginal 28dim rather closely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wU7vQl4lCiOe"
      },
      "outputs": [],
      "source": [
        "# Lets plot a scatter between original input and the\n",
        "# \"predicted\" input after the encoding/decoding steo\n",
        "\n",
        "# If the encoded/decoding steps would work perfectly you would get a\n",
        "# diagonal line.\n",
        "iCol = 5 # change this to values between (0-27) to plot other variables\n",
        "encoded_dat = encoder.predict(X_test)\n",
        "decoded_dat = decoder.predict(encoded_dat)\n",
        "plt.scatter(X_test[:,iCol],decoded_dat[:,iCol])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aJz0nMeGthf"
      },
      "outputs": [],
      "source": [
        "# Lets compute the R\n",
        "R2_test = r2_score(X_test,decoded_dat)\n",
        "R2_test\n",
        "# Ideally we would have a \"1\" which would mean that we have not information\n",
        "# loss trough our encoding/decoding step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrScCLeSh5PJ"
      },
      "source": [
        "# Step 3b: Estimating a simple logistic regression model using the encoded data from the Autoencoder as explantory variables\n",
        "\n",
        "Similarly as with PCA lets use the encoding and run a logit model to predict deforestation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYuofe_gOlJW"
      },
      "outputs": [],
      "source": [
        "# Similarly as in the PCA example we now use the encoded data\n",
        "# as an input for a logistic regression.\n",
        "encoded_dat = encoder.predict(X_train)\n",
        "\n",
        "# Fit a logistic regression\n",
        "modelLg = LogisticRegression(random_state=0,penalty='none',fit_intercept=True, max_iter=1000)\n",
        "# Fit the model using the training data\n",
        "modelLg.fit(encoded_dat, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sHOBhpzrwju"
      },
      "outputs": [],
      "source": [
        "# Get the predicted probabiltities\n",
        "Y_score = modelLg.decision_function(encoder.predict(X_test))\n",
        "\n",
        "printOutput(Y_score,Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQgJIAiYiTwf"
      },
      "source": [
        "# Do the same thing but now as a NN specification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7lRymfe2qOC"
      },
      "outputs": [],
      "source": [
        "# We can do the same thing but now defining this as a NN. For this we\n",
        "# define a output layer using a sigmoid activation function. During\n",
        "# traning we freez the encoding layers and only train the last layer.\n",
        "# Methodically this is exactly the same thing as running a logit model\n",
        "# using the encodings. The only thing that is different is the way we implement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vn9Y83IBieGq"
      },
      "outputs": [],
      "source": [
        "# this is the size of our encoded representations\n",
        "encoding_dim = 8\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "print('Encoding Dim is equal to:', encoding_dim)\n",
        "print('Input Dim is equal to:', input_dim)\n",
        "\n",
        "# Specify an output layer that uses our encoded layer from above. And has a\n",
        "# sigmoid activation. This is exactly equal then running a logit model\n",
        "# using the encoded input as we did above.\n",
        "output = Dense(1, activation='sigmoid')(encoded)\n",
        "\n",
        "# this model maps an input to its reconstruction\n",
        "NN = Model(input_dat, output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAwpLlGJk-Js"
      },
      "outputs": [],
      "source": [
        "# Freeze the encoding layer such that only the output layer is trained\n",
        "NN.layers[1].trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb3rSsSDtK4H"
      },
      "outputs": [],
      "source": [
        "# Have a look how the model looks like\n",
        "# - The hidden layer is our encoding layer from above with 232 (those weights\n",
        "#   we froze such that they are not trainable)\n",
        "# - Our last layer has only 9 parameters, this is a logit model with 8\n",
        "#   explantory variables (our encoding) and a constant\n",
        "NN.summary()\n",
        "\n",
        "# Make sure you understand the parallels to the logit model above.\n",
        "# This should illustrate that you can interprete a NN (and this holds of\n",
        "# basically all NN) as a model that does an encoding in all the layers up to\n",
        "# the last and then in the last layer a logit model takes this encoding and\n",
        "# maps it to the output!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGTWt9d7i-xZ"
      },
      "outputs": [],
      "source": [
        "# Compile model, this time using an other optimizer and \"crossentropy\" as the\n",
        "# Loss function\n",
        "# Similarly as above, we normally would tune the setting here\n",
        "NN.compile(optimizer='adam', loss='binary_crossentropy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AFcKdFgxtqE"
      },
      "outputs": [],
      "source": [
        "# load the tensorboard extension\n",
        "# tensorbord enables tracking experiment metrics like loss and accuracy, visualizing the model graph---\n",
        "%load_ext tensorboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPJGJnGg0BXN"
      },
      "outputs": [],
      "source": [
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C09EJgCezsvS"
      },
      "outputs": [],
      "source": [
        "# inspect and understand your model runs and graphs with TensorBoard\n",
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSuBeV-JjS13"
      },
      "outputs": [],
      "source": [
        "# Train only the last layer, i.e. fit the logit model\n",
        "from keras.callbacks import TensorBoard\n",
        "import datetime, os\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "history = NN.fit(X_train, Y_train,\n",
        "                epochs=30,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(X_test, Y_test),\n",
        "                callbacks=[tensorboard_callback]\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUYHa9r_sKfC"
      },
      "outputs": [],
      "source": [
        "# Get the predicted probabiltities\n",
        "Y_score_NN = NN.predict(X_test).ravel()\n",
        "\n",
        "printOutput(Y_score_NN,Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djRFBq5LDWyB"
      },
      "outputs": [],
      "source": [
        "# To rather \"proof\" that the two aproaches are ideed the same\n",
        "# lets compare the coefficients estimated in the logit model...\n",
        "modelLg.coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7V3iSqTDl7J"
      },
      "outputs": [],
      "source": [
        "# ... with the weights estimated in the last layer of the NN\n",
        "NN.layers[-1].weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywX3cj1PsXtC"
      },
      "source": [
        "# Lab NN: Train an NN end-to-end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnwZdwgBEiUk"
      },
      "outputs": [],
      "source": [
        "# Now lets setup a NN that directly takes the input and predicts deforestations.\n",
        "# Lets train this NN end-to-end.\n",
        "\n",
        "# Define you NN\n",
        "# Note: All the parts you need to solve this task are above. You only need\n",
        "# to copy the right pices from above.\n",
        "# ==============\n",
        "# Your code here\n",
        "# ==============\n",
        "\n",
        "...\n",
        "\n",
        "NN = Model( .. , ... )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r71tIhl2EmkS"
      },
      "outputs": [],
      "source": [
        "# Compile model\n",
        "# ==============\n",
        "# Your code here\n",
        "# ==============\n",
        "NN.compile(...)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yn-KFBryEprT"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "\n",
        "# ==============\n",
        "# Your code here\n",
        "# ==============\n",
        "history = NN.fit(...)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AiWOAo-s2Ql"
      },
      "outputs": [],
      "source": [
        "# Get the predicted probabiltities\n",
        "Y_score_NN = NN.predict(X_test).ravel()\n",
        "\n",
        "printOutput(Y_score_NN,Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R8E62YzmXc8"
      },
      "source": [
        "# Lab NN: Undercomplete NN\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqd5UCdTNBCm"
      },
      "outputs": [],
      "source": [
        "# In the lecture and in the code above we only consider \"undercomplete\"\n",
        "# Autoencoder, where the hidden layer has fewer layers as the input.\n",
        "# We can also train Autoencoder with hidden layers that have more neuors\n",
        "# then the input or serveral hidden layers (i.e. deep autoencoder). In order\n",
        "# to learn sensible encoding in this case when then need to add some form of\n",
        "# regularization otherwise the encoding we simple be the equal to the inputs.\n",
        "# You can find an example for Autoencoders with sparcity constraints here:\n",
        "# https://blog.keras.io/building-autoencoders-in-keras.html\n",
        "\n",
        "# Try if you can improve the performance of our autoencoder above, by\n",
        "# implementing such a sparse autoencoder.\n",
        "\n",
        "\n",
        "\n",
        "# ==============\n",
        "# Your code here\n",
        "# ==============\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.7.11 ('ml')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "b2dd1c5c1941d22dfbdf86f16c96d8db5c09a3d6da608d7809bd79814f897e15"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}