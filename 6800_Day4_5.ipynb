{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agpo-ilr-uni-bonn/PromotionskollegModule6800_2024/blob/master/6800_Day4_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71QCGinKTAAf"
      },
      "source": [
        "# Day 4-5: Code used during lecture and lab assignment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRwTIIF1s764"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "- The notebook combines 'code used during lecture' with the corresponding lab assignment (see further down)\n",
        "- Please add answers/discussion/comments to the notebook as comments or text box. Do not create another file in addition.\n",
        "- When you are done with your assignment, save the notebook in drive and add your last name to the name of the file.\n",
        "- To hand in the final notebook follow the instructions provided by email\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBjm-4D-Tg-L"
      },
      "source": [
        "# Code used during lecture: Causal Forest synthetic data\n",
        "\n",
        "__General idea of this section:__\n",
        "In this section we create synthetic data where we have a continous treatment and treatment effect are heterogenous. Treatment hererogeneity depends on only one variable, but we give the model multiple variables to check if it can identify which variables impacts treatment heterogeneity. Additionally, we check if the model can estimate the heterogeneity correctly.  \n",
        "\n",
        "\n",
        "The example is based on: https://github.com/py-why/EconML/blob/main/notebooks/Causal%20Forest%20and%20Orthogonal%20Random%20Forest%20Examples.ipynb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the econml package.\n",
        "# See documentation https://econml.azurewebsites.net/index.html\n",
        "!pip install -q econml"
      ],
      "metadata": {
        "id": "u5TIkrHvRqAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main imports\n",
        "from econml.orf import DMLOrthoForest, DROrthoForest\n",
        "from econml.dml import CausalForestDML\n",
        "from econml.sklearn_extensions.linear_model import WeightedLassoCVWrapper, WeightedLasso\n",
        "import shap\n",
        "\n",
        "# Helper imports\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "from sklearn.linear_model import Lasso, LogisticRegression\n",
        "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "jpXpr_3YRvyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We use the data generating process (DGP) from [here](https://arxiv.org/abs/1806.03467). The DGP is described by the following equations:\n",
        "\n",
        "\\begin{align}\n",
        "T =& W \\beta + \\eta, & \\;\\eta \\sim \\text{Uniform}(-1, 1)\\\\\n",
        "Y =& T\\cdot \\theta(X) + W \\gamma + \\epsilon, &\\; \\epsilon \\sim \\text{Uniform}(-1, 1)\\\\\n",
        "W \\sim& \\text{Normal}(0,\\, I_{n_w})\\\\\n",
        "X \\sim& \\text{Uniform}(0,1)^{n_x}\n",
        "\\end{align}\n",
        "\n",
        "where $W$ is a matrix of high-dimensional confounders and $\\beta, \\gamma$ have high sparsity.\n",
        "\n",
        "For this DGP,\n",
        "\\begin{align}\n",
        "\\theta(x) = \\exp(2\\cdot x_1).\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "ZQLMaoc_emlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Treatment effect function\n",
        "# Note this function is used to define the treatment effect heterogeneity,\n",
        "# if you pass a matrix x only the only the first column of the matrix\n",
        "# influences treatment heterogeneity\n",
        "def exp_te(x):\n",
        "    # Only first column in X actually affects treatment heterogeneity\n",
        "    return np.exp(2*x[0])\n",
        "\n",
        "# Set a random seed for reproducability\n",
        "np.random.seed(123)\n",
        "# Number of observations\n",
        "n = 1000\n",
        "# Number of potential covariates in W\n",
        "n_w = 30\n",
        "# Number of covariates that actually impact Y\n",
        "support_size = 5\n",
        "# Number of covariates to consider for treatment heterogeneity\n",
        "n_x = 3\n",
        "\n",
        "# Select randomly which variables i.e. columns of W actually impact Y\n",
        "support_Y = np.random.choice(range(n_w), size=support_size, replace=False)\n",
        "# Generate coefficients gamma\n",
        "coefs_Y = np.random.uniform(0, 1, size=support_size)\n",
        "\n",
        "# Treatment support, select which variables out of the support from Y\n",
        "# that impacts T)\n",
        "support_T = support_Y\n",
        "# Generate coefficients beta\n",
        "coefs_T = np.random.uniform(0, 1, size=support_size)\n",
        "\n",
        "# Generate controls, covariates, treatments and outcomes\n",
        "W = np.random.normal(0, 1, size=(n, n_w))\n",
        "X = np.random.uniform(0, 1, size=(n, n_x))\n",
        "\n",
        "# Sample random uniform errors\n",
        "epsilon = np.random.uniform(-1, 1, size=n)\n",
        "eta = np.random.uniform(-1, 1, size=n)\n",
        "\n",
        "# Derive heterogeneous treatment effects using function exp_te(x)\n",
        "TE = np.array([exp_te(x_i) for x_i in X])\n",
        "# Define treatment\n",
        "T = W[:, support_T] @ coefs_T + eta\n",
        "# Define outcome Y\n",
        "Y = TE * T + W[:, support_Y] @ coefs_Y + epsilon\n"
      ],
      "metadata": {
        "id": "KaZxR93ER5-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "estSyn = CausalForestDML(model_y=RandomForestRegressor(),\n",
        "                       model_t=RandomForestRegressor(),\n",
        "                       n_estimators=4000, min_samples_leaf=5,\n",
        "                       max_depth=50,\n",
        "                       verbose=0, random_state=123)\n",
        "# Tune the model\n",
        "estSyn.tune(Y, T, X=X, W=W)\n",
        "# Fit the model\n",
        "estSyn.fit(Y, T, X=X, W=W)\n"
      ],
      "metadata": {
        "id": "zWH9LjNxR8k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check performance of second stage model (Treatment selection model)\n",
        "estSyn.models_t[0][0].score(np.concatenate([X,W],axis=1),T)"
      ],
      "metadata": {
        "id": "nn1xEMO3bEtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check performance of second stage model (Outcome model)\n",
        "estSyn.models_y[0][0].score(np.concatenate([X,W],axis=1),Y)"
      ],
      "metadata": {
        "id": "goxFD7Eqa_lS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define test data, for plots below\n",
        "X_test = np.array(list(product(np.arange(0, 1, 0.01), repeat=n_x)))"
      ],
      "metadata": {
        "id": "hsWg4EL5cFAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "treatment_effects = estSyn.effect(X_test)\n",
        "# Calculate default (95%) confidence intervals for the test data\n",
        "te_lower, te_upper = estSyn.effect_interval(X_test, alpha=0.01)"
      ],
      "metadata": {
        "id": "yVDRaJfAcgEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = estSyn.effect_inference(X_test)"
      ],
      "metadata": {
        "id": "WEUIrbMLSCaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Have a look at all the estimated treatment effects\n",
        "res.summary_frame().head()\n"
      ],
      "metadata": {
        "id": "V_OZ7eZ5fDYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate mean treatment effect estimates for X_test\n",
        "print(res.summary_frame()['point_estimate'].mean())\n",
        "print(np.mean(treatment_effects))"
      ],
      "metadata": {
        "id": "6HP_OFyFfdQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mean_point is the mean treatment effect estimate\n",
        "res.population_summary()"
      ],
      "metadata": {
        "id": "Lq5vaEfFfWQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the estimated treatment heterogeneity with respect to X[:,0]\n",
        "Xi = 0\n",
        "plt.subplot()\n",
        "plt.title(\"CausalForest\")\n",
        "plt.plot(X_test[:, Xi], treatment_effects, label='mean')\n",
        "expected_te = np.array([exp_te(x_i) for x_i in X_test])\n",
        "plt.plot(X_test[:, Xi], expected_te, 'b--', label='True effect')\n",
        "plt.fill_between(X_test[:, Xi], te_lower, te_upper, label=\"95% BLB CI\", alpha=0.3)\n",
        "plt.ylabel(\"Treatment Effect\")\n",
        "plt.xlabel(f\"x{Xi}\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jmPDRS2NSEOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Feature Importance for treatment heterogeneity')\n",
        "pd.DataFrame(estSyn.feature_importances_,index=[f'x{i}' for i in range(0,n_x)],\n",
        "             columns=['feature_importances']\n",
        "             ).sort_values('feature_importances',ascending=False)"
      ],
      "metadata": {
        "id": "9NuC5qUoSMEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get shap values\n",
        "shap_values = estSyn.shap_values(X)"
      ],
      "metadata": {
        "id": "9l3vnpI7SILT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.beeswarm(shap_values['Y0']['T0'])"
      ],
      "metadata": {
        "id": "6Awlq9JTSJ_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZm0tXJ8DLDM"
      },
      "source": [
        "# Lab 4a: Counterfactual prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czQzN5tNDZua"
      },
      "source": [
        "The objective of the second part of today's lab is to determine the effects of protected areas on forest cover. To do this, we take the following strategy:\n",
        "\n",
        "1) We estimate a model predicting forest cover where we use all of the observations that are not in a protected area.\n",
        "\n",
        "2) We use this model to predict forest cover for observations with protected areas observations. This gives as a prediction about the expected forest cover given the characteristics of the cell (such as slope, elevation).\n",
        "\n",
        "3) Using the predictions from 2) we compare the predicted forest cover with the actual forest cover. The difference between the two is the estimated effect of forest protected areas.\n",
        "\n",
        "=> Most of the steps for the estimation should be familiar from previous labs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9dV8MX8FRjV"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "# import seaborn for visualization\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urcpRQs9QFxO"
      },
      "outputs": [],
      "source": [
        "# run this cell only once if you don't have wget installed\n",
        "# its assumed you are using windows and have python installed\n",
        "# only needed if you are running the notebook locally\n",
        "# %pip install wget\n",
        "#if not os.path.isfile('brazil_all_data_v2.gz'):\n",
        "#    !python -m wget  https://ilr-ml.s3.eu-central-1.amazonaws.com/brazil_all_data_v2.gz\n",
        "# Download data only once and make sure it is in the same folder as the notebook\n",
        "\n",
        "# check if brazil_all_data_v2.gz is available in the current folder and if not, download it\n",
        "\n",
        "if not os.path.isfile('brazil_all_data_v2.gz'):\n",
        "    !wget  https://ilr-ml.s3.eu-central-1.amazonaws.com/brazil_all_data_v2.gz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXsLE6cBwJlT"
      },
      "outputs": [],
      "source": [
        "# load data into dataframe\n",
        "df = pd.read_parquet('brazil_all_data_v2.gz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxNVCmreJT9M"
      },
      "outputs": [],
      "source": [
        "# Define target (dependent) variable (% forest cover for 2018)\n",
        "strY = 'perc_treecover'\n",
        "\n",
        "# Define a list of features names (explantory variables)\n",
        "lstX = [\n",
        "  # 'wdpa_2017', => Exclude protected areas\n",
        "  'population_2015',\n",
        "  'chirps_2017',\n",
        "  'maize',\n",
        "  'soy',\n",
        "  'sugarcane',\n",
        "  'perm_water',\n",
        "  'travel_min',\n",
        "  'cropland',\n",
        "  'mean_elev',\n",
        "  'sd_elev',\n",
        "  'near_road',\n",
        "  # 'wdpa_2017_lag_1st_order',\n",
        "  'chirps_2017_lag_1st_order',\n",
        "  'population_2015_lag_1st_order',\n",
        "  'maize_lag_1st_order',\n",
        "  'soy_lag_1st_order',\n",
        "  'sugarcane_lag_1st_order',\n",
        "  'perc_treecover_lag_1st_order',\n",
        "  'perm_water_lag_1st_order',\n",
        "  'travel_min_lag_1st_order',\n",
        "  'cropland_lag_1st_order',\n",
        "  'mean_elev_lag_1st_order',\n",
        "  'sd_elev_lag_1st_order',\n",
        "  'near_road_lag_1st_order',\n",
        " ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koS5lAM3I9m7"
      },
      "outputs": [],
      "source": [
        "# Split that sample in those observations that ...\n",
        "#... are protected areas\n",
        "df_PA = df.loc[~(df['wdpa_2017']==0),:]\n",
        "\n",
        "#... are NOT protected areas\n",
        "df_NoPA = df.loc[df['wdpa_2017']==0,:]\n",
        "\n",
        "print('Number of observations without protected area', df_NoPA.shape[0])\n",
        "print('Number of observations with protected area', df_PA.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgJR5As0FDgu"
      },
      "outputs": [],
      "source": [
        "# Select the target variable\n",
        "# ... for the observations without protected areas\n",
        "Y_all_NoPA = df_NoPA[strY]\n",
        "# ... for the observations with protected areas\n",
        "Y_PA = df_PA[strY]\n",
        "\n",
        "# Get the features\n",
        "# ... for the observations without protected areas\n",
        "X_all_NoPA =  df_NoPA.loc[:,lstX]\n",
        "# ... for the observations wit protected areas\n",
        "X_PA_raw =  df_PA.loc[:,lstX]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kH38Z7Ufk-I"
      },
      "source": [
        "Choose a ML model and features to predict deforestation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePR5RYQ9ftsL"
      },
      "outputs": [],
      "source": [
        "# Split the data into train and test using only the the observations\n",
        "# without protected areas\n",
        "X_train_raw, X_test_raw, Y_train, Y_test = train_test_split(X_all_NoPA, Y_all_NoPA, test_size = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcujfVW9E_kb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Scale data to 0-1 range using sklearn MinMaxScalar object\n",
        "# (see: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n",
        "scaler = MinMaxScaler()\n",
        "# Use only the train data to fit the MinMaxScalar\n",
        "scaler.fit(X_train_raw)\n",
        "\n",
        "# Apply the MinMax transformation to the train and test data\n",
        "X_train = scaler.transform(X_train_raw)\n",
        "X_test = scaler.transform(X_test_raw)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-IGittsEV46"
      },
      "outputs": [],
      "source": [
        "# Define a function to print model stats\n",
        "def printModStats(mod,X_train, Y_train,X_test, Y_test, showPlot=True):\n",
        "  # Inspect the model performancefor those observations that are\n",
        "  # not protected areas\n",
        "  print('Score in train', mod.score(X_train, Y_train))\n",
        "  print('Score in test', mod.score(X_test, Y_test))\n",
        "\n",
        "  # Get predicted values for the test set\n",
        "  Y_test_had_Tree = mod.predict(X_test)\n",
        "\n",
        "  # Calculate MSE in test set\n",
        "  mse_ols_sklearn  = mean_squared_error(Y_test,Y_test_had_Tree)\n",
        "  print('\\nMean squared error: ',mse_ols_sklearn)\n",
        "  # The coefficient of determination: 1 is perfect prediction\n",
        "  R2_ols_sklearn = r2_score(Y_test,Y_test_had_Tree)\n",
        "  print('Coefficient of determination: ',R2_ols_sklearn)\n",
        "\n",
        "  if showPlot:\n",
        "    # plot Y vs Y-hat\n",
        "    h = sns.jointplot(Y_test_had_Tree, Y_test, kind=\"hex\")\n",
        "    h.set_axis_labels('Y predicted', 'Y true');\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HKK5AhGf6eI"
      },
      "outputs": [],
      "source": [
        "# Run XGBoost. In contrast to day 2 we now use XGBRegressor as our task is\n",
        "# a regression task and not a classification task\n",
        "import xgboost as xgb\n",
        "model_xgb = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42)\n",
        "model_xgb.fit(X_train, Y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqIcCCsKEmX_"
      },
      "outputs": [],
      "source": [
        "printModStats(model_xgb,X_train, Y_train,X_test, Y_test, showPlot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSLkW7g224nj"
      },
      "source": [
        "Now we use the trained model to make predictions for those areas with protected areas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LW40wwuk6joa"
      },
      "outputs": [],
      "source": [
        "# First we need to scale the data for those observations without\n",
        "# protected areas\n",
        "X_PA = scaler.transform(X_PA_raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_wtagGgJwmN"
      },
      "outputs": [],
      "source": [
        "# Lets define a function for our conterfactual that can be reused below\n",
        "def calculateConterfactual(mod,X_PA):\n",
        "  # Then use the train model to make predictions for protected areas\n",
        "  Y_PA_had = mod.predict(X_PA)\n",
        "\n",
        "  # Now we can compare the mean forest cover in protected area that we observer\n",
        "  print(f'Predicted mean forest cover in protected Areas {np.mean(Y_PA_had):0.2f} %' )\n",
        "  print(f'Actual mean forest cover in protected Areas {np.mean(Y_PA):0.2f} %', )\n",
        "  print(f'Mean difference between predicted and observed forest cover: {np.mean(Y_PA-Y_PA_had):0.2f} pp')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Djf7N-zj7M37"
      },
      "outputs": [],
      "source": [
        "# Call function for our xgb model\n",
        "calculateConterfactual(model_xgb,X_PA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pxCQ8okFsQU"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "#  Task\n",
        "# ================================\n",
        "\n",
        "# Until now we have always used the default hyperparameters for XGBoost\n",
        "# Lets see if we can improve model performance by optimizing some of those.\n",
        "# Usually this is a rather complex and time consuming task with many\n",
        "# different ways to approach this. Here we only try to optimize two paramters\n",
        "# manually. Specifically we want to optimize \"max_depth\" (Typical values: 3-10)\n",
        "# and \"min_child_weight\" (Typical values: 1-5).\n",
        "\n",
        "# Hint: The caculation is quite time consuming. Coordinate in your team\n",
        "#       to try as many combination as possible.\n",
        "\n",
        "# In case you are interested you can find a complete guide for doing\n",
        "# hyperparameter optimization for XGB here:\n",
        "# https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
        "# This is NOT required for this task here, just to give you an idea of how\n",
        "# many paramter can be optimized (Note, this referes to an older sklearn version,\n",
        "# therefore some of the code might need to be adjusted)\n",
        "\n",
        "# Specify model with \"max_depth\" and \"min_child_weight\" set explicitly\n",
        "model_xgb_opt = ...\n",
        "\n",
        "# Fit model to train data\n",
        "model_xgb_opt....\n",
        "\n",
        "# Print stats\n",
        "printModStats(model_xgb_opt,X_train, Y_train,X_test, Y_test, showPlot=True)\n",
        "\n",
        "# Do conterfactual\n",
        "calculateConterfactual(model_xgb_opt,X_PA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsNLu-jzHKOk"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "#  Optional Task\n",
        "# ================================\n",
        "\n",
        "# Try alternative ML models and check how they perform compared to XGBoost.\n",
        "# If you are using sklearn you often only have to replace one line in oder to\n",
        "# change your model. LightGBM for example is an alterantive to XGBoost that\n",
        "# gained popularity.\n",
        "# https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html#lightgbm.LGBMRegressor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0v6H7tP8Oq8"
      },
      "source": [
        "## Alternative approach\n",
        "\n",
        "- Train a model with \"protected areas\" included\n",
        "- Select only the protected areas (in the test set)\n",
        "- make a prediction for those areas for forest cover\n",
        "- make a prediction for those areas for forest cover where we set the explanatory variable for \"protected areas\" equal to zero i.e. we make a prediction for forest cover under the assumption that those areas where not protected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyIRMR3X4xCk"
      },
      "outputs": [],
      "source": [
        "# Define binary variable for deforestration in 2018\n",
        "\n",
        "Y_all = df['perc_treecover']\n",
        "# Define a list of features names (explantory variables)\n",
        "lstX = [\n",
        "   'wdpa_2017',\n",
        "  'population_2015',\n",
        "  'chirps_2017',\n",
        "  'maize',\n",
        "  'soy',\n",
        "  'sugarcane',\n",
        "  'perm_water',\n",
        "  'travel_min',\n",
        "  'cropland',\n",
        "  'mean_elev',\n",
        "  'sd_elev',\n",
        "  'near_road',\n",
        "  'wdpa_2017_lag_1st_order',\n",
        "  'chirps_2017_lag_1st_order',\n",
        "  'population_2015_lag_1st_order',\n",
        "  'maize_lag_1st_order',\n",
        "  'soy_lag_1st_order',\n",
        "  'sugarcane_lag_1st_order',\n",
        "  'perc_treecover_lag_1st_order',\n",
        "  'perm_water_lag_1st_order',\n",
        "  'travel_min_lag_1st_order',\n",
        "  'cropland_lag_1st_order',\n",
        "  'mean_elev_lag_1st_order',\n",
        "  'sd_elev_lag_1st_order',\n",
        "  'near_road_lag_1st_order',\n",
        " ]\n",
        "\n",
        "# Get the explanatory Variables\n",
        "X_all =  df.loc[:,lstX]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1KM91dB5pwy"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Split the data into train and test using all observations\n",
        "\n",
        "# Use only the train data to fit the MinMaxScalar\n",
        "\n",
        "\n",
        "# Apply the MinMax transformation to the train and test data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42uzaA5-5xMQ"
      },
      "outputs": [],
      "source": [
        "# train...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDyt913g8F5-"
      },
      "outputs": [],
      "source": [
        "# Get only those observations which are protected areas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6wk-utf9AwZ"
      },
      "outputs": [],
      "source": [
        "# do prediction for these as if they were not protected areas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAeCcDPG8iJZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chjqTO7h81qp"
      },
      "outputs": [],
      "source": [
        "# And compare with the case they are protected\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kMcZPeZ_s5i"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Now we can compare the mean forest cover in protected area that we observer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JNWwRkK9lRV"
      },
      "outputs": [],
      "source": [
        "# => Discuss pro and cons of the two approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab b: Causal Forest for deforestration example"
      ],
      "metadata": {
        "id": "gwcFQEF5RdZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task: The code below illustrate how CausalForest could be used for the deforestration example (in a very simplified way).\n",
        "Before you run the code below think about how\n",
        "\n",
        "1) soy\n",
        "\n",
        "2) mean_elev\n",
        "\n",
        "3) near_road\n",
        "\n",
        "might effect treatment heterogeneity. Write down your hypothesis in the notebook below (e.g., I expecte the treatement effect to go down with the mean elevation because...). Note, that there is not one single right answer to this. The idea is that you think about the expected effect before actually running the model.\n",
        "  "
      ],
      "metadata": {
        "id": "bIEEyt3VirLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####################\n",
        "### Your Task #######\n",
        "#####################\n",
        "# Your hypothesis for \"soy\"\n",
        "\"\"\"\n",
        "Your answer here:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5rka8v5goJrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################\n",
        "### Your Task #######\n",
        "#####################\n",
        "# Your hypothesis for \"mean_elev\"\n",
        "\"\"\"\n",
        "Your answer here:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "S81Az8CloNOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################\n",
        "### Your Task #######\n",
        "#####################\n",
        "# Your hypothesis for \"near_road\"\n",
        "\"\"\"\n",
        "Your answer here:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "fHZ1S0uSoNWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now run the code below"
      ],
      "metadata": {
        "id": "RnBwSRD9lHTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the econml package (if not already done above).\n",
        "# See documentation https://econml.azurewebsites.net/index.html\n",
        "!pip install -q econml"
      ],
      "metadata": {
        "id": "JhV2eTzlO6I4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main imports\n",
        "from econml.orf import DMLOrthoForest, DROrthoForest\n",
        "from econml.dml import CausalForestDML\n",
        "from econml.sklearn_extensions.linear_model import WeightedLassoCVWrapper, WeightedLasso\n",
        "import shap\n",
        "import xgboost as xgb\n",
        "\n",
        "# Helper imports\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "from sklearn.linear_model import Lasso, LogisticRegression\n",
        "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "-u8KCQS-SOgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run this cell only once if you don't have wget installed\n",
        "# its assumed you are using windows and have python installed\n",
        "# only needed if you are running the notebook locally\n",
        "# %pip install wget\n",
        "#if not os.path.isfile('brazil_all_data_v2.gz'):\n",
        "#    !python -m wget  https://ilr-ml.s3.eu-central-1.amazonaws.com/brazil_all_data_v2.gz\n",
        "# Download data only once and make sure it is in the same folder as the notebook\n",
        "\n",
        "# check if brazil_all_data_v2.gz is available in the current folder and if not, download it\n",
        "\n",
        "if not os.path.isfile('brazil_all_data_v2.gz'):\n",
        "    !wget  https://ilr-ml.s3.eu-central-1.amazonaws.com/brazil_all_data_v2.gz"
      ],
      "metadata": {
        "id": "VWR2oa3nSP_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data with pandas into a dataframe\n",
        "df = pd.read_parquet('brazil_all_data_v2.gz')"
      ],
      "metadata": {
        "id": "-oYaWsunSRhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define binary variable for deforestration in 2018\n",
        "Y_all = df['defor_2018']"
      ],
      "metadata": {
        "id": "NBLZ7oSlSSze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of features names (explantory variables)\n",
        "\n",
        "# This are the variables you include to derive treatment heterogeneity\n",
        "lstX = [\n",
        "  'soy',\n",
        "  'mean_elev',\n",
        "  'near_road',\n",
        " ]\n",
        "# This are the variables you include as controll variables\n",
        "lstW = [\n",
        "  # 'wdpa_2017',\n",
        "  'population_2015',\n",
        "  'chirps_2017',\n",
        "  'defor_2017',\n",
        "  'maize',\n",
        "  # 'soy',\n",
        "  'sugarcane',\n",
        "  'perc_treecover',\n",
        "  'perm_water',\n",
        "  'travel_min',\n",
        "  'cropland',\n",
        "  # 'mean_elev',\n",
        "  'sd_elev',\n",
        "  # 'near_road',\n",
        "  'defor_2017_lag_1st_order',\n",
        "  # 'wdpa_2017_lag_1st_order',\n",
        "  'chirps_2017_lag_1st_order',\n",
        "  'population_2015_lag_1st_order',\n",
        "  'maize_lag_1st_order',\n",
        "  'soy_lag_1st_order',\n",
        "  'sugarcane_lag_1st_order',\n",
        "  'perc_treecover_lag_1st_order',\n",
        "  'perm_water_lag_1st_order',\n",
        "  'travel_min_lag_1st_order',\n",
        "  'cropland_lag_1st_order',\n",
        "  'mean_elev_lag_1st_order',\n",
        "  'sd_elev_lag_1st_order',\n",
        "  'near_road_lag_1st_order',\n",
        " ]\n",
        "\n",
        "# Make sure that variables in lstX are not in lstW\n",
        "lstW = list(set(lstW)-set(lstX))\n",
        "\n",
        "X_all =  df.loc[:,lstX]\n",
        "W_all =  df.loc[:,lstW]\n",
        "T_all = df['wdpa_2017']\n"
      ],
      "metadata": {
        "id": "89DVNS9DSUSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test data using sklearn train_test_split object\n",
        "#   (see: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
        "\n",
        "# Note: This randomly split the data in 80% train and 20% test data\n",
        "X_train_raw, X_test_raw,W_train_raw, W_test_raw, T_train, T_test, Y_train, Y_test = train_test_split(X_all,W_all,T_all, Y_all, test_size = 0.2)"
      ],
      "metadata": {
        "id": "a9gtKn-MSXJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale data to 0-1 range using sklearn MinMaxScalar object\n",
        "# (see: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n",
        "scalerX = MinMaxScaler()\n",
        "scalerW = MinMaxScaler()\n",
        "# Use only the train data to fit the MinMaxScalar\n",
        "scalerX.fit(X_train_raw)\n",
        "scalerW.fit(W_train_raw)\n",
        "\n",
        "# Apply the MinMax transformation to the train and test data\n",
        "X_train = scalerX.transform(X_train_raw)\n",
        "X_test = scalerX.transform(X_test_raw)\n",
        "W_train = scalerW.transform(W_train_raw)\n",
        "W_test = scalerW.transform(W_test_raw)\n",
        "# Note the depended variable does not need to be scaled as it is a binary variable anyway\n",
        "# Similarly, the treatment variables is between 0-1 and hence does not need to be scaled\n"
      ],
      "metadata": {
        "id": "LQpTBwKPSYy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now train a CausalForest mode, actually what are using here\n",
        "# is a combination of a CausalForest and \"Double Machine Learning\".\n",
        "# Double machine Learning allows to control for non-linear effect of W on\n",
        "# T and Y (by stripping out those effect) and then only estimate treatment\n",
        "# heterogeneity with respect to X\n",
        "\n",
        "# Note1: We substaintially reduce the number of estimators and the max_depth\n",
        "#        to reduce runtime. For an actual application you might want to\n",
        "#        consider higher values\n",
        "est1 = CausalForestDML(\n",
        "                       model_y=xgb.XGBRegressor(),\n",
        "                       model_t=xgb.XGBRegressor(),\n",
        "                       #  n_estimators=4000, min_samples_leaf=5,\n",
        "                       #  max_depth=50,\n",
        "                       n_estimators=100, min_samples_leaf=5,\n",
        "                       max_depth=5,\n",
        "                       verbose=0, random_state=123)\n",
        "# Tune the model (see documentation:\n",
        "# https://econml.azurewebsites.net/_autosummary/econml.dml.CausalForestDML.html#econml.dml.CausalForestDML.tune)\n",
        "# Note2: We also restrict the size of the training data in the next tuning step\n",
        "#        to reduce runtime. In an actual application you might not want to\n",
        "#        do this.\n",
        "# est1.tune(Y_train, T_train, X=X_train, W=W_train)\n",
        "est1.tune(Y_train[:20000], T_train[:20000], X=X_train[:20000], W=W_train[:20000])"
      ],
      "metadata": {
        "id": "YD9zELhPb1GN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "est1.fit(Y_train, T_train, X=X_train, W=W_train)"
      ],
      "metadata": {
        "id": "r_GFbwmb1wAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: model_y and model_t are estimated multiple times using cross-validation\n",
        "#       est1.model_t containes a nested list of all the fitted models\n",
        "#       (default value for cross-validation is 2, hence there are two fitted\n",
        "#       instances in the list)\n",
        "print('Score test set model t (instance 0)',est1.models_t[0][0].score(np.concatenate([X_test,W_test],axis=1),T_test))\n",
        "print('Score test set model t (instance 1)',est1.models_t[0][1].score(np.concatenate([X_test,W_test],axis=1),T_test))\n",
        "print('Score test set model y (instance 0)',est1.models_y[0][0].score(np.concatenate([X_test,W_test],axis=1),Y_test))\n",
        "print('Score test set model y (instance 1)',est1.models_y[0][1].score(np.concatenate([X_test,W_test],axis=1),Y_test))"
      ],
      "metadata": {
        "id": "YBaG5-FntAVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################\n",
        "### Your Task #######\n",
        "#####################\n",
        "# What are these two numbers telling you\n",
        "\"\"\"\n",
        "Your answer here:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "W3q-LypGolR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Feature Importance for treatment heterogeneity')\n",
        "pd.DataFrame(est1.feature_importances_,index=lstX,columns=['feature_importances']).sort_values('feature_importances',ascending=False)"
      ],
      "metadata": {
        "id": "H9d6wnY2ScvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################\n",
        "### Your Task #######\n",
        "#####################\n",
        "# How do you interpret the above table. Explain in you own words.\n",
        "\"\"\"\n",
        "Your answer here:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5PebUj4hnsDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate SHAP values for the first 1000 test samples\n",
        "shap_values = est1.shap_values(X_test[:10000],feature_names=lstX)\n"
      ],
      "metadata": {
        "id": "2FdMWHhSSeUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot SHAP values (for treatment heterogeneity)\n",
        "shap.plots.beeswarm(shap_values['defor_2018']['wdpa_2017'])"
      ],
      "metadata": {
        "id": "DwCb4wI_uISI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################\n",
        "### Your Task #######\n",
        "#####################\n",
        "# How do you interpret the plot above. Relate this to the hypothesis you\n",
        "# defined above\n",
        "\"\"\"\n",
        "Your answer here:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "dNV-ohHIn5oE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO think about if this is actually useful\n",
        "shap.plots.scatter(shap_values['defor_2018']['wdpa_2017'], color=shap_values['defor_2018']['wdpa_2017'])"
      ],
      "metadata": {
        "id": "6OVFFwQd3r76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO think about if this is actually useful\n",
        "sample_ind = 0\n",
        "shap.plots.waterfall(shap_values['defor_2018']['wdpa_2017'][sample_ind])"
      ],
      "metadata": {
        "id": "dVJgP26T3ZwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UeNrRFg2RnO6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.7.11 ('ml')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "b2dd1c5c1941d22dfbdf86f16c96d8db5c09a3d6da608d7809bd79814f897e15"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}